{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is an interdisciplinary field concerned with the interactions between computers and human natural languages — speech or text. NLP-powered softwares help us in our daily lives in various ways, for example:\n",
    "\n",
    "* Personal Assistants\n",
    "\n",
    "* Auto-Complete\n",
    "\n",
    "* Spell checking\n",
    "\n",
    "* Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are generally 3 different types of NLP systems:\n",
    "\n",
    "* Rule-based systems: domain-specific rules (e.g: regular expressions), can be used to solve simple problems such as extracting structured data (e.g: emails) from unstructured data (e.g: web-pages)\n",
    "\n",
    "* Classical ML approaches: uses crafted features fed into a statistical ml model which learns patterns in the training set and applies them to previously unseen data. (e.g: Spam Detection)\n",
    "\n",
    "* Deep Learning approaches: use feature extractors in an automatic way, allowing for models that provide end-to-end solutions, allowed hard nlp tasks to be completed (e.g: Machine Translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoding: where a sentence is represented as a matrix of shape (NxN) where N is the number of unique tokens in the sentence, for example in the above picture, each word is represented as a sparse vectors (mostly zeroes) except of one cell (could be one, or the number of occurrences of the word in the sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however, this approach has two major drawbacks: \n",
    "    \n",
    "* memory capacity issues (sparse vectors)\n",
    "* lack of meaning represenation (no similarities between words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result a majority of modern nlp algorithms will use word2vec, a shallow deep learning approach which represents words as dense vectors, and allow capturing of semantic meaning.\n",
    "\n",
    "Further research built upon word2vec to create models like GloVe and fastText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Classification of Text Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses a text message spam dataset from Kaggle which can be found here: https://www.kaggle.com/uciml/sms-spam-collection-dataset/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"spam.csv\", encoding = \"latin-1\")\n",
    "data = data[['v1', 'v2']]\n",
    "data = data.rename(columns = {'v1': 'label', 'v2': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this dataset is pretty unbalanced which we will have to deal with later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to regular data cleaning there is a much heavier emphasis on text normalization because it makes it much easier to extract semantic meaning. Also is important because it reduces heavily the amount of computational power we need since the matrix is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some methods include:\n",
    "\n",
    "* Case normalization\n",
    "\n",
    "* Removing stop words, NOTE: There is a lot of debate over when removing stop words is a good idea. This practice is used in many information retrieval tasks (such as search engine querying), but can be detrimental when syntactical understanding of language is required.\n",
    "\n",
    "* Removing punctuations, special symbols\n",
    "\n",
    "* Lemmatising/Stemming: reducing inflection forms to normalise words with the same lemma or base. (note that lemmatising differs from stemming in that it considers the context of the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other normalization techniques include error correction, converting words to their parts of speech or mapping to synonyms, many of the tools to do this are included in the nltk library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular problem, only case normalization is used because stemmers are difficult to apply to colloquial english and removing stop words will reduce the size of already short texts even more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize \n",
    "def review_messages(msg):\n",
    "    # converting messages to lowercase\n",
    "    msg = msg.lower()\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference on how to do stemming and remove stopwords\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def alt_review_messages(msg):\n",
    "    # converting messages to lowercase\n",
    "    msg = msg.lower()\n",
    "    # removing stopwords\n",
    "    msg = [word for word in msg.split() if word not in stopwords]\n",
    "    # using a stemmer\n",
    "    msg = \" \".join([stemmer.stem(word) for word in msg])\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(review_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words model discussed earlier, is too simple for this task so we will use the TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For review, this is how the TF-IDF vectorizer (Term Frequency — Inverse Document Frequency) works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF vectorizes documents by calculating a TF-IDF statistic between the document and each term in the vocabulary. The document vector is constructed by using each statistic as an element in the vector.\n",
    "\n",
    "![title](pics/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After settling with TF-IDF, we must decide the granularity of our vectorizer. A popular alternative to assigning each word as its own term is to use a tokenizer. A tokenizer splits documents into tokens (thus assigning each token to its own term) based on white space and special characters.\n",
    "\n",
    "For example, the phrase what’s going on might be split into what, ‘s, going, on.\n",
    "\n",
    "However since we are dealing with colloquial English with possibly URLs and emails, we are splitting by word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size = 0.1, random_state = 1)\n",
    "# training the vectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wesley~1\\docume~1\\qmi\\presen~1\\nlp\\nlpenv\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "from sklearn import svm\n",
    "svm = svm.SVC(C=1000)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[490   0]\n",
      " [ 10  58]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import confusion_matrix\n",
    "X_test = vectorizer.transform(X_test)\n",
    "y_pred = svm.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9964285714285716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_score =svm.decision_function(X_test)\n",
    "print(roc_auc_score(y_test, y_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(msg):\n",
    "    msg = vectorizer.transform([msg])\n",
    "    prediction = svm.predict(msg)\n",
    "    return prediction[0]\n",
    "\n",
    "data['pred'] = data['text'].apply(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  pred\n",
       "0   ham  go until jurong point, crazy.. available only ...   ham\n",
       "1   ham                      ok lar... joking wif u oni...   ham\n",
       "2  spam  free entry in 2 a wkly comp to win fa cup fina...  spam\n",
       "3   ham  u dun say so early hor... u c already then say...   ham\n",
       "4   ham  nah i don't think he goes to usf, he lives aro...   ham"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Trained over large corpora, word2vec uses unsupervised learning to determine semantic and syntactic meaning from word co-occurrence, which is used to construct vector representations for every word in the vocabulary.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more detailed look into how it works is included here: https://arxiv.org/pdf/1301.3781.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses a two layer shallow neural network to find the vector mappings for each word in the corpus. The neural network is used to predict known co-occurrences in the corpus and the weights of the hidden layer are used to create the word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two model architectures used to train word2vec: Continuous Bag of Words and Skip Gram. These models determine how textual data is passed into the neural network.\n",
    "\n",
    "Both of these architectures use a context window to determine contextually similar words. \n",
    "\n",
    "A context window with a fixed size n means that all words within n units from the target word belong to its context.\n",
    "Consider the following example with a fixed window size of 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fox is our target word and quick, brown, jumped, over belong to the context of fox. The assumption is that with enough examples of contextual similarity, the network is able to learn the correct associations between words.\n",
    "This assumption is in line with the distributional hypothesis which states that “words which are used and occur in the same contexts tend to purport similar meaning”\n",
    "\n",
    "The implementation of context window in word2vec is dynamic.\n",
    "\n",
    "A dynamic context window has a maximum window size. Context is sampled from the maximum window size with probability 1/d, where d is the distance between the word to the target.\n",
    "\n",
    "Consider the target word fox using a dynamic context window with maximum window size of 2. (brown, jumped) have a 1/1 probability of being included in the context since they are one word away from fox. (quick, over) have a 1/2 probability of being included in the context since they are two words away from fox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words\n",
    "We structure the data such that the context is used to predict the target word. For example, if our context is (quick, brown, jumped, over), we use that as features of the class fox.\n",
    "### Skip Gram\n",
    "We structure the data such that the target word is used to predict the context. For example, we use the feature (fox) to predict the context (quick, brown, jumped, over)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network\n",
    "Word2vec trains a shallow neural network over data as structured using either Continuous Bag of Words or Skip Gram architecture. Instead of leveraging the model for predictive purposes, we use the hidden weights from the neural network to generate the word vectors.\n",
    "\n",
    "Assuming a Continuous Bag of Words architecture with a fixed context window of 1 word, this is what the process would look like. First, the corpus.\n",
    "\n",
    "##### very simple corpus\n",
    "\n",
    "\n",
    "**I like math.**\n",
    "\n",
    "**I like programming.**\n",
    "\n",
    "**Today is Friday.**\n",
    "\n",
    "**Today is a good day.**\n",
    "\n",
    "To make things even easier, we can require our context window to only include words which proceeds the target. We can assume that the context of words at the end of a sentence is the first word of the next sentence. Under such rules:\n",
    "\n",
    "* like is the context of target I\n",
    "\n",
    "* math is the context of target like\n",
    "\n",
    "* programming is also the context of target like\n",
    "\n",
    "Even with such a simple corpus, we can begin to recognize some patterns. “Math” and “programming” are both context to “like”. While this might not be picked up by the model, both of these words can be understood as things that I like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "### 1. One-hot encode\n",
    "### 2. feed forward neural-network with one hidden layer and a output using a softmax activation function\n",
    "\n",
    "The data set used to train the network uses the one hot encoded context vector to predict the one hot encoded target vector.\n",
    "The number of neurons in the hidden layer corresponds to the number of dimensions in the final word vectors.\n",
    "\n",
    "### 3. Obtain weights of hidden network\n",
    "Each row in the weight matrix corresponds to the vector of each word in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your own word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import nltk \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "chess = wikipedia.page(\"Chess\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521\n"
     ]
    }
   ],
   "source": [
    "# split our document into sentences\n",
    "sentences = nltk.sent_tokenize(chess) \n",
    "\n",
    "length = len(sentences)\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "for i in range(0, length): \n",
    "    \n",
    "    # further tokenize our sentences\n",
    "    temp = nltk.word_tokenize(sentences[i])\n",
    "    \n",
    "    # removing stop words, non-alpabetical tokens and converting to lower case \n",
    "    sentences[i] = [word.lower() for word in temp if word not in stopwords and word.isalpha()]    \n",
    "\n",
    "print(len(sentences))\n",
    "    \n",
    "# size refers to the desired dimensionality of vectors \n",
    "# window is upper bound in dynamic context window\n",
    "# sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "model = Word2Vec(sentences, size=100, window=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48693413"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measures the similarity between words using cosine similarity \n",
    "\n",
    "model.wv.similarity(\"rook\", \"knight\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chess', 0.7700669765472412),\n",
       " ('player', 0.7547798752784729),\n",
       " ('world', 0.753699541091919),\n",
       " ('players', 0.7380087375640869),\n",
       " ('example', 0.7331035733222961),\n",
       " ('rating', 0.7260173559188843),\n",
       " ('pawns', 0.716291069984436),\n",
       " ('moves', 0.7120612859725952),\n",
       " ('game', 0.7079309821128845),\n",
       " ('two', 0.6990262866020203)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds the top n most similar words \n",
    "\n",
    "model.wv.similar_by_word(\"king\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('opponent', 0.9239256978034973),\n",
       " ('checkmate', 0.8886814117431641),\n",
       " ('chess', 0.8274511098861694),\n",
       " ('pawns', 0.7916924357414246),\n",
       " ('player', 0.7825618982315063),\n",
       " ('game', 0.7560703158378601),\n",
       " ('pawn', 0.7550085186958313),\n",
       " ('world', 0.7383760809898376),\n",
       " ('rating', 0.7352045774459839),\n",
       " ('games', 0.7286425232887268)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find word using vector addition. Does opponent + checkmate = lose? \n",
    "\n",
    "opponent_checkmate = model.wv[\"opponent\"] + model.wv[\"checkmate\"] # add the vectors for king and checkmate\n",
    "model.wv.most_similar(positive = [opponent_checkmate], topn=10) # find most similar word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1064163e-04,  2.7849227e-03, -6.9686084e-04, -7.6698628e-04,\n",
       "       -1.8941686e-03, -8.2237376e-03,  8.1017101e-04, -3.6570632e-03,\n",
       "        5.4781986e-03, -4.7286502e-03, -6.1999485e-03, -2.3176360e-03,\n",
       "        4.3832846e-03, -1.2438551e-04,  1.6759310e-03,  8.2134837e-03,\n",
       "        7.0482595e-03, -4.8480411e-03, -1.2123109e-03, -1.0787294e-02,\n",
       "        6.2058652e-03,  2.9616610e-03,  6.8436926e-03,  4.7568060e-03,\n",
       "       -4.5343186e-03,  9.4778789e-04,  4.5285113e-03, -3.1439899e-03,\n",
       "        8.4937172e-04, -2.2017551e-03, -6.4727659e-03,  2.2153649e-03,\n",
       "       -4.6007079e-03,  4.4411896e-03, -9.0642720e-03,  7.5955037e-04,\n",
       "       -7.1106087e-03,  4.8968929e-04, -4.3056291e-03, -7.3126878e-04,\n",
       "       -1.7485697e-03, -3.7330389e-03, -5.8895075e-03, -4.4027744e-03,\n",
       "        2.6723740e-03,  1.1313351e-02, -3.6093348e-03, -3.0519129e-04,\n",
       "       -4.9488023e-03,  2.4059690e-03, -1.4644397e-03, -3.9526285e-03,\n",
       "       -2.7676583e-03, -8.5032859e-04,  2.0749362e-03, -2.9554041e-03,\n",
       "       -8.2297744e-03,  1.1056616e-02, -1.2142867e-03,  7.1436597e-04,\n",
       "       -7.1977396e-05, -1.7706503e-03, -1.5081858e-03, -8.0683418e-03,\n",
       "        1.0242681e-03, -2.9544253e-03, -1.0487422e-03,  5.9620378e-04,\n",
       "        3.0042194e-03, -2.1515866e-03, -5.3867726e-03,  1.6452522e-03,\n",
       "       -4.4882922e-03, -1.8160100e-03, -4.4436585e-03,  8.6731110e-03,\n",
       "       -3.7487752e-03,  2.4904837e-03,  2.9429549e-03,  4.5461608e-03,\n",
       "       -1.1151667e-03,  9.0746926e-03,  3.4912943e-03,  2.6217391e-04,\n",
       "       -4.6608099e-03,  7.6953636e-04,  7.9740724e-03,  2.6731188e-03,\n",
       "        2.7198740e-03, -1.1278596e-02,  8.8129105e-04, -1.8863027e-03,\n",
       "        1.8593550e-03,  7.1553313e-03,  6.4340113e-03,  4.3139071e-03,\n",
       "       -9.3941297e-04,  2.1879438e-03, -1.1881458e-03,  5.5847145e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try doing this using a larger corpus and see how it can work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have contructed a word2vec model you can use it in several ways to plug it into your standard ml models\n",
    "\n",
    "Average of Word2Vec vectors : You can just take the average of all the word vectors in a sentence. This average vector will represent your sentence vector.\n",
    "\n",
    "Average of Word2Vec vectors with TF-IDF : Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
